
# These results are slightly out of date: since then I changed
# the LDA+MLLT to use 7, not 9 frames of context, and also increased
# the learning rate for the "indirect" fMMI.

for x in exp/{mono,tri,sgmm,nnet}*/decode*; do [ -d $x ] && grep Sum $x/score_*/*.sys | utils/best_wer.sh; done 2>/dev/null
for x in exp/{mono,tri,sgmm,nnet}*/decode*; do [ -d $x ] && grep WER $x/wer_* | utils/best_wer.sh; done 2>/dev/null

exit 0


# Note: the 2nd to last number is the WER.
exp/tri1/decode_eval2000/score_11/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 57.0   31.6   11.4    4.7   47.8   75.7 |
exp/tri2/decode_eval2000/score_12/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 56.9   31.1   11.9    4.4   47.4   75.3 |

exp/tri3a/decode_eval2000/score_12/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 60.3   28.2   11.4    3.9   43.5   74.3 |

exp/tri4a/decode_eval2000/score_12/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 67.2   23.7    9.1    3.9   36.7   70.3 |
exp/tri4a/decode_train_dev/wer_13:%WER 35.60 [ 17252 / 48460, 1946 ins, 4232 del, 11074 sub ]
exp/tri4a/decode_eval2000.si/score_11/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 59.3   29.8   11.0    4.4   45.2   75.2 |
exp/tri4a/decode_train_dev.si/wer_13:%WER 45.38 [ 21989 / 48460, 2154 ins, 5464 del, 14371 sub ]

exp/tri5a/decode_eval2000/score_12/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 71.2   21.0    7.8    3.8   32.6   68.7 |

exp/sgmm2_5a/decode_eval2000/score_10/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 74.9   17.7    7.5    3.1   28.3   64.6 |
exp/sgmm2_5a_mmi_b0.1/decode_eval2000_it1/score_10/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 76.2   16.9    7.0    3.1   27.0   63.7 |
exp/sgmm2_5a_mmi_b0.1/decode_eval2000_it2/score_10/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 76.7   16.6    6.6    3.2   26.4   63.2 |
exp/sgmm2_5a_mmi_b0.1/decode_eval2000_it3/score_10/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 77.1   16.4    6.5    3.2   26.1   63.0 |
exp/sgmm2_5a_mmi_b0.1/decode_eval2000_it4/score_10/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 77.4   16.3    6.3    3.4   26.0   62.8 |
exp/sgmm5a/decode_eval2000/score_9/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 74.1   18.5    7.4    3.5   29.4   66.3 |
exp/sgmm5a_mmi_b0.1/decode_eval2000_it1/score_10/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 75.1   17.6    7.3    3.2   28.0   64.9 |
exp/sgmm5a_mmi_b0.1/decode_eval2000_it2/score_9/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 76.3   17.3    6.5    3.4   27.1   64.1 |
exp/sgmm5a_mmi_b0.1/decode_eval2000_it3/score_9/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 76.9   17.0    6.1    3.5   26.6   63.7 |
exp/sgmm5a_mmi_b0.1/decode_eval2000_it4/score_9/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 77.2   16.9    5.9    3.6   26.4   63.4 |



exp/tri5a_dnn/decode_eval2000/score_10/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 77.6   15.5    7.0    2.8   25.2   62.9 |
exp/tri5a_dnn/decode_train_dev/wer_10:%WER 22.85 [ 11075 / 48460, 1261 ins, 2955 del, 6859 sub ]

# Dan's neural net setup (I need to tune this a but-- it was working better before.)
# Note: I've found the problem, I was using too few leaves in the SAT baseline.
# I'm fixing it now.  (This should help Karel's setup too.)
exp/nnet5c/decode_eval2000/score_9/eval2000.ctm.filt.sys:     | Sum/Avg    | 4459  42989 | 76.0   16.8    7.2    2.7   26.7   64.0 |
exp/nnet5c/decode_train_dev/wer_8:%WER 26.06 [ 12627 / 48460, 1891 ins, 2891 del, 7845 sub ]
