# Copyright 2016    Vincent Renkens
#                   Gaofeng Cheng

[directories]
# directory where the training features will be stored and retrieved
train_features = data
# directory where the testing features will be stored and retrieved
test_features = data
# directory where the all the data from this experiment will be stored (logs, models, ...)
expdir = exp

[general]
# command to specific gpu card id you'd like to use,
# gmm ali number store file
gmm_ali_number = gmm_ali_number
# for this stage, tenssorflow under kaldi only supports one gpu card
gpu_card_id = 1
# command used for kaldi
cmd = /nobackup/datapool/project/proa/chenggaofeng/wsj/s5/utils/run.pl


[dnn-features]
# name of the features. If you want to use the GMM features, give it the same name
name = train_si284_hires
test_feature_name = test_dev93_hires,test_eval92_hires

[nnet]
# name of the neural net
name = 2048_6_relu_si284_hires_cgf
# name of the gmm model used for the alignments
gmm_ali_name = tri4b_ali_si284
gmm_name = tri4b
# lang model used for decoding
graph_dir = graph_tgpr,graph_bd_tgpr
#size of the left and right context window
context_width = 5
# number of neurons in the hidden layers
num_hidden_units = 2048
# number of hidden layers
num_hidden_layers = 6
# the network is initialized layer by layer. This parameters determines the frequency of adding layers. 
# Adding will stop when the total number of layers is reached. Set to 0 if no layer-wise initialisation is required
add_layer_period = 50
# starting step, set to 'final' to skip nnet training
starting_step = 0
# if you're using monophone alignments, set to True
monophone = False
# nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = relu
# if you want to do l2 normalization after every layer set to 'True'
l2_norm = True
# if you want to use dropout set to a value smaller than 1
dropout = 1
# Flag for using batch normalisation
batch_norm = False
# number of passes over the entire database
num_epochs = 10
# initial learning rate of the neural net
initial_learning_rate = 0.001
# exponential weight decay parameter
learning_rate_decay = 1
# size of the minibatch (#utterances)
batch_size = 128
# to limit memory ussage (specifically for GPU) the batch can be devided into even smaller batches. The gradient will be calculated by averaging the gradients of all these mini-batches. 
# This value is the size of these mini-batches in number of frames. For optimal speed this value should be set as high as possible without exeeding the memory. To use the entire batch set to -1
numframes_per_batch = 4096
# size of the validation set, set to 0 if you don't want to use one
valid_batches = 2
# frequency of evaluating the validation set
valid_frequency = 10
# if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = True
# number of times the learning will retry (with half learning rate) before terminating the training
valid_retries = 3
# how many steps are taken between two checkpoints
check_freq = 10
# you can visualise the progress of the neural net with tensorboard
visualise = True


