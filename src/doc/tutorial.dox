// doc/tutorial.dox

// Copyright 2009-2011 Microsoft Corporation

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//  http://www.apache.org/licenses/LICENSE-2.0

// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
// MERCHANTABLITY OR NON-INFRINGEMENT.
// See the Apache 2 License for the specific language governing permissions and
// limitations under the License.

/**
 \page tutorial Kaldi tutorial

  \section tutorial_prereqs Prerequisites

  This tutorial assumes that you know the basics of speech recognition using the
  HMM-GMM approach.  One brief introduction that is available online
  is: M. Gales and S. Young (2007). ``The Application of Hidden Markov
  Models in Speech Recognition." Foundations and Trends in Signal Processing
  1(3): 195-304.  The HTK Book is also a good resource.  However, unless you
  have a strong mathematical background and are extremely dedicated, we
  discourage trying to learn about speech recognition outside an institutional
  setting.  The intended audience for this tutorial is either speech recognition
  researchers, or graduates or advanced undergraduates who are studying this
  area anyway.

  We assume that you know C++, and have at least some familiarity with shell
  scripting, preferably using bash or a similar shell.  This tutorial assumes you
  are using a UNIX-like environment or Cygwin (although Kaldi will not
  necessarily compile and run in all such environments).

  Also, importantly, the tutorial assumes you have access to the data on the Resource
  Management (RM) CDs from the Linguistic Data Consortium (LDC), in the original form
  as distributed by the LDC.  That is, we assume this data is sitting on your system
  somewhere.  We obtained this as catalog number LDC93S3A.   It is
  also available in two separate pieces.  Be careful because there was previously 
  a different distribution of the RM data with a different layout.

  The system requirements are fairly basic.  We assume that you have tools
  including wget, svn, awk, perl and so on, or that you know how to install them.
  The most difficult part of the installation process relates to the math library
  ATLAS; if this is not already installed as a library on your system you will
  have to compile it, and this requires that CPU throttling be turned off, which
  may require root priveleges.  We provide scripts and detailed instructions for
  all installation steps.  When scripts fail, read the output carefully because
  it tries to provide guidance as to how to fix problems.  Please inform us if there
  are problems at any point, however minor; see \ref contact.

  We try to provide some idea how long it should take to execute each step of the tutorial.
  If there is a limited amount of time available to complete the tutorial, we recommend
  to try to keep to the posted schedule, if necessary by skipping steps and avoiding
  following links to more information that we provide in the text.  This will help ensure
  that you get a balanced overview.  You can always review the material in more
  detail later on.  If this tutorial is to be given in a classroom setting, it is 
  important that someone run through the tutorial on the relevant system beforehand in order
  to verify that all the prerequisites are installed.

  \section tutorial_setup Getting started (15 minutes)

  The first step is to download and install Kaldi.  We will be using version 1 of
  the toolkit, so that this tutorial does not get out of date.  Assuming
  Subversion (svn) is installed, type:
  \verbatim
    svn co https://kaldi.svn.sourceforge.net/svnroot/kaldi/branches/1 kaldi-1
  \endverbatim
  Then cd to kaldi-1.  Look at the INSTALL file and follow the instructions 
  (it points you to two subdirectories).  Look carefully at the output of the
  installation scripts, as they try to guide you what to do.  Some installation
  errors are non-fatal, and the installation scripts will tell you so (i.e. there 
  are some things it installs which are nice to have but are not really needed).
  The "best-case" scenario is that you do:
 \verbatim
   cd tools/; ./install.sh; cd ../src; ./configure; make
 \endverbatim
 and everything will just work; however, if this does not happen there are
 fallback plans (e.g. you may have to install some package on your machine, or run 
 install_atlas.sh in tools/, or run some steps in tools/INSTALL manually, 
 or provide options to the configure script in src/).  If there are problems,
 there may be some information in \ref build_setup that will help you; otherwise,
 feel free to contact the maintainers (\ref contact) and we will be happy to help.  


  \section tutorial_svn Version control with Subversion (3 minutes)

 In case you are unfamiliar with the Subversion (svn) version control system, we
 give a brief overview of some commands that might be useful to you.  Subversion commands
 always look like: "svn [command] [arguments]"; you can do "svn help" to see what
 commands are available, or "svn help <command>" for help on a specific command.  
  In kaldi-1 or any subdirectory, type
 \verbatim
   svn up
 \endverbatim
 (this is short for "svn update").  If we have committed changes to the repository
 in the several minutes since you installed Kaldi, you should see output like
 the following:
\verbatim
kaldi-1: svn update
U    src/lat/Makefile
U    src/nnetbin/nnet-forward.cc
Updated to revision 191.
\endverbatim
 More likely, it will just say something like "At revision 191."  
 To see if you have made any changes to anything, type "svn status".  This will
 list files that you changed or that have been added.  Files that have been added
 to the directories but are not under version control because you have not used the
 "svn add" command, will appear with the descriptor '?' (you will see all the
 binaries that were compiled).  If you are going to be 
 contributing to the Kaldi project (and we do welcome new contributors), 
 then you should become familiar with other commands such
 as "svn add", "svn commit" and so on.  For this, there are tutorials available
 online.
 
 \section tutorial_looking Overview of the distribution (15 minutes)

 Before we jump into the example scripts, let us take a few minutes to look at what
 else is included in the Kaldi distribution.  Go to the kaldi-1 directory and list it.
 There are a few files and subdirectories.  
 The important subdirectories are "tools/", "src/", and "egs/" which we will
 look at in the next section.
 We will give an overview of the first two.

 \subsection tutorial_looking_tools The tools/ directory (5 minutes)

 The directory "tools/' is where we install things that Kaldi depends on in
 various ways.  Change directory to tools/ and list it.  You will see various
 files and subdirectories, mostly things that have been installed by the script
 install.sh.  Look very quickly at the files install.sh and INSTALL.  These files
 are similar since they cover the same steps, but INSTALL is the manual version
 of the instructions.  The manual version may be helpful if you have installation problems.

 The most important subdirectory is the one for OpenFst.  cd to openfst/.  This is a soft link
 to the actual directory which has a version number.  List the openfst directory.
 If the installation succeeded, there will be a bin/ directory with the installed
 binaries, and a lib/ directory with the library (we require both of these).
 The most important code is in the directory include/fst/.  If you ever want to
 understand Kaldi deeply you will need to understand OpenFst.  For this,
 the best starting point is http://www.openfst.org/.

 For now, just view the file include/fst/fst.h.  This consists of some declarations
 of an abstract FST type.  You can see that there are a lot of templates involved.
 If templates are not your thing, you will probably have trouble understanding this code.

 Change directory to bin/, or add it to your path.  
 We will be executing some simple example instructions from
 <a href=http://www.openfst.org/twiki/bin/view/FST/FstQuickTour#CreatingFsts>here</a>.

 Paste the following commands into the shell:
\verbatim
# arc format: src dest ilabel olabel [weight]
# final state format: state [weight]
# lines may occur in any order except initial state must be first line
# unspecified weights default to 0.0 (for the library-default Weight type) 
cat >text.fst <<EOF
0 1 a x .5
0 1 b y 1.5
1 2 c z 2.5
2 3.5
EOF
\endverbatim

The following commands create the symbol tables; paste them into the shell too.
\verbatim
cat >isyms.txt <<EOF
<eps> 0
a 1
b 2
c 3
EOF

cat >osyms.txt <<EOF
<eps> 0
x 1
y 2
z 3
EOF
\endverbatim
Next create a binary-format FST:
\verbatim
fstcompile --isymbols=isyms.txt --osymbols=osyms.txt text.fst binary.fst
\endverbatim
Let's execute an example command:
\verbatim
fstinvert binary.fst | fstcompose - binary.fst > binary2.fst
\endverbatim
The resulting WFST, binary2.fst, should be similar to binary.fst
but with twice the weights.  You can print them both out to see:
\verbatim
fstprint --isymbols=isyms.txt --osymbols=osyms.txt binary.fst
fstprint --isymbols=isyms.txt --osymbols=osyms.txt binary2.fst
\endverbatim
This example was modified from a longer tutorial available at
<a href=www.openfst.org> www.openfst.org </a>.


 \subsection tutorial_looking_src The src/ directory

 Change directory back up to the top level (kaldi-1) and into src/.
 List the directory.  You will see a few files and a large number of 
 subdirectories.  Look at the Makefile.  At the top it sets the variable
 SUBDIRS.  This is a list of the subdirectories containing code.
 Notice that some of them end in "bin".  These are the ones that contain
 executables (the code and executables are in the same directory).  The
 other directories contain internal code.

 You can see that one of the targets in the Makefile is "test".
 Type "make test".  This command goes into the various subdirectories and
 runs test programs in there.  All the tests should succeed.  If you are
 feeling lucky you can also type "make valgrind".  This runs the same
 tests with a memory checker, and takes longer, but will find more 
 errors.  If this doesn't work, forget about it.
 
 Change directory to (for example) base/.  Look at the Makefile.  Notice the line
\verbatim
include ../kaldi.mk
\endverbatim
 This lines includes the file ../kaldi.mk verbatim whenever a Makefile in a
 subdirectory is invoked.  Look at the file ../kaldi.mk.  It will contain
 some rules related to valgrind (for memory debugging), and then some
 system-specific configuration in the form of variables such as CXXFLAGS.
 See if there are any -O options (e.g. -O0).  You might want to remove the flags
 -O0 and -DKALDI_PARANOID before running big experiments, as they slow things
 down (we enable them by default for better debugging).
 
Going back to the Makefile in the base/ directory: choose one of the binaries
listed in TESTFILES, and run it.  Then briefly view the corresponding .cc file.
The math one is a good example (note: this excludes the majority of math functions
in Kaldi, which are matrix-vector related functions, and are located in
../matrix/).  Notice that there are a lot of assertions, with the macro
KALDI_ASSERT.  These test programs are designed to exit with error status if
there is a problem (they are not supposed to rely on human inspection of the
output).

Look at the header kaldi-math.h.  You will see some elements of our coding practices.
Notice that all our \#includes are relative to the src/ directory (so we \#include
base/kaldi-types.h even though we are already in the base/ directory).
Notice that all macros we \#define, except for standard ones that we are just
making sure have their normal values, begin with KALDI_.  This is a precaution
to avoid future conflicts with other codebases (since \#defines don't limit themselves
to the kaldi namespace).  Notice the style of the function names: LikeThis().  
Our style is generally based on
<a href=http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml> this one </a>,
to conform with OpenFst, but there are some differences.

To see other elements of the style, which will help you to understand Kaldi
code, cd to ../util, and view text-utils.h.  Notice that the inputs of these
functions are always first, and are generally const references, while the
outputs (or inputs that are modified) are always last, and are pointer arguments.  Non-const references
as function arguments are not allowed.  You can read more about the Kaldi-specific
elements of the coding style \ref style "here" later if you are interested.  
For now, just be aware that there is a coding style with quite specific rules.

Change directory to ../gmmbin and type
\verbatim
./gmm-init-model 
\endverbatim
It prints out the usage, which should give you a generic idea of how Kaldi programs
are called.  Note that while there is a --config option that can be used to
pass a configuration file, in general Kaldi is not as config-driven as HTK and these
files are not widely used.  You will see a --binary option.  In general, Kaldi file
formats come in both binary and test forms, and the --binary option controls how
they are written.  However, this only controls how single objects (e.g. acoustic models)
are written.  For whole collections of objects (e.g. collections of feature files), 
there is a different mechanism that we will come to later.
Type
\verbatim
./gmm-init-model >/dev/null
\endverbatim
What do you see, and what does this tell you about what Kaldi does with logging-type
output?  The place that the usage message goes is the same place that all error and
logging messages go, and there is a reason for this, which should become apparent
when you start looking at the scripts.
 
To get a little insight into the build process,  cd to ../matrix, and type
\verbatim
rm *.o
make
\endverbatim
Look at the options that are passed to the compiler.  These are ultimately
controlled by the variables that are set in ../kaldi.mk, which in turn is
determined by ../configure.  Also look at the linking options, passed in when it
creates matrix-lib-test.  You will get some idea what math libraries it is
linking against (this is somewhat system dependent).  For more information on how
we make use of external matrix libraries, you can read \ref matrixwrap.

Change directory to one level up (to src/), and look at the "configure" file.  If you
are familiar with the "configure" files generated by automake, you will notice that it
is not one of those.  It is hand generated.  Search within it for "makefiles/"
and quickly scan all the places where that string occurs (e.g. type into the shell
"less configure", type "/makefiles[enter]" and then type "n" to see later instances).
You will see that it makes use of some files with the .mk in the subdirectory "makefiles/".
These are essentially "prototype" versions of kaldi.mk.  Look at one of the prototypes,
e.g. makefiles/cygwin.mk, to see the kinds of things they contain.  For systems that are more
predictable, it just copies the  corresponding file to kaldi.mk.  For Linux, it has to
do a little more sleuthing because there are so many distributions.  Mostly this relates
to finding where the math libraries are installed.  If you are having problems with
a build process, we recommend modifying kaldi.mk by hand.  In order to do this you should
probably understand how Kaldi makes use of external math libraries (see \ref matrixwrap).

\section tutorial_running Running the example scripts for Resource Management

\subsection tutorial_running_start Getting started, and prerequisites.

The next stage of the tutorial is to start running the example scripts for
Resource Management.  Change directory to the top level (we called it kaldi-1),
and then to egs/.  Look at the README.txt file in that directory, and
specifically look at the Resource Management section.  It mentions the LDC
catalog number corresponding to the corpus.  This may help you in obtaining the
data from the LDC.  If you cannot get the data for some reason, just continue
reading this tutorial and doing the steps that you can do without the data, and
you may still obtain some value from it.  The best case is that there is some
directory on your system, say /mnt/data4/RM, that contains three subdirectories;
call them rm1_audio1, rm1_audio2 and rm2_audio.  These would correspond to the
three original disks in the data distribution from the LDC, with no changes in
format.  These instructions assume your shell is bash.  If you have a different
shell, these commands will not work or should be modified (just type "bash"
to get into bash, and everything should work).


Now change directory to rm/, glance at the file README.txt to see what the
overall structure is, and cd to s1/.  This is the basic sequence of experiments
that corresponds to the main functionality in version 1 of the toolkit.

In s1/, list the directory and glance at the RESULTS file so you have some
idea what is in there (later on, you should verify that the results you get
are similar to what is in there).  The main file we will be looking at
is run.sh.  Note: run.sh is not intended to be run directly from the shell;
the idea is that you run the commands in it one by one, by hand.

\subsection tutorial_running_data_prep Data preparation

The first three lines you have to run are as follows [assuming your data is
in /mnt/data4/RM, which is an example]:
\verbatim
cd data_prep
./run.sh /mnt/data4/RM
cd ..
\endverbatim
If this works it should say "Succeeded".  If not, you will have to work out
where the script failed and what the problem was.

The next couple of steps in run.sh copy some things from data_prep/ into data/.
Run these.  The general concept behind this directory structure is that
data_prep/ contains the corpus-specific data preparation stages and data/
contains the same data in some kind of "normalized" form, so that in theory the
same set of system-building works could run from the same data/ directory.
However, it is not perfectly executed and we will improve the organization of the
scripts at some point.

At this point let's have a quick look at the things that were prepared
in data_prep/.  Change directory to data_prep/, and run the following
commands, looking at the output each time.
\verbatim
 # G.txt is the word-pair grammar supplied with the RM corpus.
 head G.txt   
 head lexicon.txt
 head train_trans.txt
 head train_wav.scp
\endverbatim
These will give you some idea of what the outputs of a generic data preparation process would
look like.  Something you should appreciate is that not all of these are "native" Kaldi
formats:
 - The grammar, G.txt, needs to be compiled into an FST before it is used, and even then
   it is not read directly by Kaldi but is processed further with OpenFst tools before
   being used.
 - The lexicon, lexicon.txt, needs to be converted by OpenFst into the binary FST format
   before Kaldi will read it (and we'll turn the words and phones into integer labels;
   Kaldi will only deal with integers).
 - The transcriptions, train_trans.txt, will also be turned into an integer format
   (but still text),  with the words replaced with integers.
 - The file train_wav.scp is actually read directly by Kaldi programs.  Look at the file
   again.  It is parsed as a set of key-value pairs, where the key is the first string on
   each line.  The value is a kind of "extended filename", and you can guess how it works.  
   Since it is for reading we will refer to this type of string as an "rxfilename" (for writing
   we use the term wxfilename).  See \ref io_sec_xfilename if you are curious.  Note that
   although we use the extension .scp, this is not a script file in the HTK sense (i.e. it
   is not viewed as an extension to the command-line arguments). 

Now, from the directory s1/, run the next step in run.sh which is:
\verbatim
 steps/prepare_graphs.sh
\endverbatim
Look at the script.  It transforms some of the files created in data_prep/ to a more
normalized form that is read by Kaldi.  This script creates its output in the
data/ directory.  The files we mention below will be in that directory.

The first two files this script creates are called words.txt and phones.txt.
These are OpenFst format symbol tables, and represent a mapping from strings to
integers and back.

Look at the files with suffix .csl.  These are colon-separated lists of
the integer id's of non-silence, and silence, phones respectively.  They are sometimes
needed as options on program command lines (e.g. to specify lists of silence phones).

Look at phones_disambig.txt.  This file is a phone symbol table that also 
handles the "disambiguation symbols" used in the standard FST recipe.
These symbols are conventionally called \#1, \#2 and so on;
 see the paper <a href=www.cs.nyu.edu/~mohri/pub/hbka.pdf> "Speech Recognition
with Weighted Finite State Transducers" </a>.  We also add a symbol \#0
which replaces epsilon transitions in the language model; see
\ref graph_disambig for more information.

The file L.fst is the compiled lexicon in FST format.  To see what kind of information
is in it, you can (from s1/), do:
\verbatim
 . path.sh
 fstprint --isymbols=data/phones_disambig.txt --osymbols=data/words.txt data/L.fst | head
\endverbatim
The FST format is not very human-readable.  Look at the command-line that creates L.fst
in steps/prepare_graphs.sh.  Try to figure out what the 0.5 on the command line means
(you will have to look at the Perl script that is invoked).

\subsection tutorial_running_feats Feature extraction

The next step is to extract the training features.  Search for "mfcc" in run.sh and
run the corresponding three lines of script (you have to decide where you want to put the 
features first and modify the example accordingly).  Suppose we decide to put the
features on /my/disk/rm_mfccdir, we would do something like:
\verbatim
mkdir /my/disk/rm_mfccdir
steps/make_mfcc_train.sh /my/disk/rm_mfccdir
steps/make_mfcc_test.sh /my/disk/rm_mfccdir
\endverbatim
Run these jobs.  They
use several CPUs in parallel and should be done in around two minutes on a fast
machine.  

In the script steps/make_mfcc_train.sh,
look at the line that invokes split_scp.pl.  By doing a word count of data_prep/train_wav.scp
and the files matched by the pattern exp/make_mfcc/train_wav?.scp, you can see what this
line does.
Next look at the line that invokes compute-mfcc-feats.  The options should be fairly self-explanatory.
The option that involves the config file is a mechanism that can be used in Kaldi to pass configuration
options, like a HTK config file, but it is quite rarely used.  The positional arguments (the
ones that begin with "scp" and "ark,scp" require a little more explanation.  

Before we explain this, have a look at the command line in the script again and examine
the inputs and outputs using:
\verbatim
head exp/make_mfcc/train_wav1.scp
head /my/disk/rm_mfccdir/train_raw_mfcc1.scp
less /my/disk/rm_mfccdir/train_raw_mfcc1.ark
\endverbatim
(in these commands you would replace /my/disk/rm_mfccdir/ with the actual directory.
Be careful-- the .ark file contains binary data (you may have to type "reset" if your terminal
doesn't work right after viewing this data).

By listing the files you can see that the .ark files are quite big (because they contain
the actual data).  You can view one of these archive files more conveniently by typing:
\verbatim
. path.sh
copy-feats ark:/my/disk/rm_mfccdir/train_raw_mfcc1.ark ark,t:- | head
\endverbatim
[From now we will omit the ". path.sh" command and assume you have already done it].
You can remove the ",t" modifier from this command and try it again if you like-- but
it might be a good to pipe it into "less" because the data will be binary.
An alternative way to view the same data is to do:
\verbatim
copy-feats scp:data/train.scp ark,t:- | head
\endverbatim
This is because the archive and script file both represent the same data.  Notice
the "scp:" and "ark:" prefixes in these commands.  Kaldi doesn't attempt to work
out whether something is a script file or archive format from the data itself,
and in fact Kaldi never attempts to work things out from file suffixes.  This is
for general philosophical reasons, and also to forestall bad interaction with
pipes (because pipes don't have a filename).

Now type the following command:
\verbatim
head -10 data/train.scp | tail -1 | copy-feats scp:- ark,t:- | head
\endverbatim
This prints out some data from the tenth training file.  Notice that in
"scp:-", the "-" tells it to read from the standard input, while "scp" tells
it to interpret the input as a script file.

Next we will describe what script and archive files actually are.
The first point we want to make is that the code sees both of them
in the same way.  For a particularly simple example of the user-level
calling code, type the following command:
\verbatim
tail -30 ../../../src/featbin/copy-feats.cc
\endverbatim

The basic concept is the concept of an ordered set of items (e.g. feature files),
indexed by strings (e.g. utterance identifiers).  We call this a Table (it is not
really a C++ object, as we have separate C++ objects to access the data depending
whether we are writing, iterating, or doing random access).  The .scp format has
lines with a key, and then an "extended filename" that tells Kaldi where to find
the data.  The archive format may be text or binary (you can write in text mode
with the ",t" modifier).  The format is: the key (e.g. utterance id), then a
space, then the object data.  Kaldi doesn't attempt to put the object type into
the archive; you have to know the object type in advance (archives can't contain
mixtures of types).  Many times, we will pipe data between programs using
archives on the standard input and output; when this happens, you will see the
string "ark:-" as one of the command line arguments.  For more details on this
topic you can see \ref io.

\subsection tutorial_running_feats Monophone training

The next step is to train monophone models.  If the disk where you installed
Kaldi is not big, you might want to make exp/ a soft link to a directory somewhere
on a big disk (if you run all the experiments and don't clean up, it can get up 
to a few gigabytes).  Type
\verbatim
nohup steps/train_mono.sh &
\endverbatim
You can view the most recent output of this by typing
\verbatim
tail nohup.out
\endverbatim
(we run longer jobs this way so they can finish running even if we get disconnected).
There is actually very little output that goes to the standard out and error of this
script; most of it goes to log files in exp/mono/.

While it is running, look at the file exp/mono/topo.  This file is created immediately.
One of the phones has a different topology from the others.  Look at data/phones.txt
in order to figure out from the numeric id which phone it is.  Notice that each entry in
the topology file has a final state with no transitions out of it.  The convention in
the topology files is that the first state is initial (with probability one) and the
last state is final (with probability one).

Type 
\verbatim
less exp/mono/0.mdl
\endverbatim
and look at the model file.  You will see that it contains the information in 
topology file at the top of it, and then some other things, before the model parameters.
The convention is that the .mdl file contains two objects: one of type TransitionModel,
and one of the relevant model type (in this case, type AmGmm).  By "contains two objects",
what we mean is that the objects have Write and Read functions in a standard form, and
we call these functions to write the objects to the file.  For objects such as this,
that are not part of a Table (i.e. there is no "ark:" or "scp:" involved), writing is
in binary or text mode and can be controlled by the standard command-line 
options --binary=true or --binary=false (different programs have different defaults).
Glance through the model file to see what kind of information it contains.  At this
point we won't go into more detail on how models are represented in Kaldi; see
hmm to find out more.  

We will mention one important point, though: p.d.f.'s in Kaldi are represented by
numeric id's, starting from zero (we call these pdf-ids).  They do not have
"names", as in HTK.  The .mdl file does not have sufficient information to map
between context-dependent phones and pdf-ids.  For the tree file, look at
exp/mono/tree.  Note that this is a monophone "tree" so it is very trivial-- it
does not have any "splits".  Although this tree format was not indended to be
very human-readable, because we have received a number of queries about this, we
will explain it.  The rest of this paragraph can be skipped over by the casual reader.
After "ToPdf", the tree file contains an object of the
polymorphic type EventMap, which can be thought of as storing a mapping from a
set of integer (key,value) pairs representing the phone-in-context and HMM state,
to a numeric p.d.f. id.  Derived from EventMap are the types ConstantEventMap
(representing the leaves of the tree), TableEventMap (representing some kind of
lookup table) and SplitEventMap (representing a tree split).  In this file
exp/mono/tree, "CE" is a marker for ConstantEventMap (and corresponds to the
leaves of the tree), and "TE" is a marker for TableEventMap (there is no "SE", or
SplitEventMap, because this is the monophone case).  "TE 0 49" is the start of a
TableEventMap that "splits" on key zero (representing the zeroth phone position
in a phone-context vector of length one, for the monophone case).  It is
followed, in parentheses by 49 objects of type EventMap.  The first one is NULL
(representing a zero pointer to EventMap) because the phone-id zero is reserved
for "epsilon").  An example non-NULL object is the string "TE -1 3 ( CE 33 CE 34
CE 35 )", which represents a TableEventMap splitting on key -1.  The value
assigned to this key can take the values 0, 1 or 2 for this phone, and
corresponds to the numeric PdfClass specified in the topology file.   In our
topology file this is identical to the HMM-state index.  Inside the parentheses are three
objects of type ConstantEventMap, each representing a leaf of the tree.

Now look at the file exp/mono/cur.ali (it should exist if the training has progressed
far enough).  This is the Viterbi alignment of the training data; it has one line
for each training file.  Now do "tail exp/mono/tree" and look for the highest-numbered
p.d.f. id (which is the last number in the file).  Compare this with the numbers in
exp/mono/cur.ali.  Does something seem wrong?  The reason is that the alignment file
does not contain p.d.f. id's.  It contains a slightly more fine-grained identifier
that we call a "transition-id".  This also encodes the phone, and the transition within
the prototype topology of the phone.  This is useful for a number of reasons.
For more details, see \ref hmm.

Next let's look at how training is progressing (this step assumes your shell is bash).
Type
\verbatim
grep Overall exp/mono/acc.{?,??}.log
\endverbatim
You can see the acoustic likelihods on each iteration.  Next look at one of the files
exp/mono/update.*.log to see what kind of information is in the update log.



\subsection tutorial_looking_whats What's in the code

Now that we have a vague idea of what the code looks like, where it is, and how it is
compiled, we are going to briefly skim over some parts of the code to give you an idea
what is in there.  

First look at the file base/kaldi-common.h.  This file includes a number of
things from the base/ directory that are used by almost every Kaldi program.  You
can mostly guess from the filenames the types of things that are provided: things
like error-logging macros, typedefs, math utility functions such as random number
generation, and miscellaneous \#defines.  But this is a stripped-down set of
utilities; look at util/common-utils.h to see a more complete set, including
command-line parsing and I/O functions that handle extended filenames such as
pipes.  Some of the other I/O related utilities included in util/common-utils.h
are hard to describe concisely, and we'll come to them in time.  The reason why
we segregated a subset of utilities into the base/ directory is so that we could
minimize the set of things that the matrix/ library depends on (since it's useful
independent of the rest of Kaldi).

Look at the file matrix/matrix-lib.h.  See what files it includes.  This provides
an overview of the kinds of things that are in the matrix library.  This library
is basically a C++ wrapper for BLAS and LAPACK, in case that means anything to you.
The files sp-matrix.h and tp-matrix relate to symmetric packed matrices and
triangular packed matrices, respectively.  Quickly scan the file matrix/kaldi-matrix.h.
This will give you some idea what the matrix code looks like.  It consists of
a C++ class representing a matrix.  We provide a mini-tutorial on the matrix
library \ref matrix "here", if you are interested.  Scanning through
matrix/matrix-lib-test.cc will give you some idea how the various matrix
and vector functions are called.

Next look at gmm/diag-gmm.h (this class stores a Gaussian Mixture Model).  
The class DiagGmm may look a bit confusing as
it has many different accessor functions.  Search for "private" and look
at the class member variables (they always end with an underscore, as per
the Kaldi style).  This should make it clear how we store the GMM.
This is just a single GMM, not a whole collection of GMMs. 
Look at gmm/am-diag-gmm.h; this class stores a collection of GMMs.
Notice that it does not inherit from anything.
Search for "private" and you can see the member variables (there
are only two of them).  You can understand from this how simple the
class is (everything else consists of various accessors and convenience
functions).  A natural question to ask is: where are the transitions,
where is the decision tree, and where is the HMM topology?  All of these
things are kept separate from the acoustic model, because it's likely
that researchers might want to replace the acoustic likelihoods while
keeping the rest of the system the same.  We'll come to all this stuff later.

Next look at feat/feature-mfcc.h.  Focus on the MfccOptions struct.
The struct members give you some idea what kind of options are supported
in MFCC feature extraction.  
Notice that some struct members are options structs themselves.
Look at the Register function.  This is standard in Kaldi options classes.
Then look at featbin/compute-mfcc-feats.cc and search for Register.
You can see where this function is called from the command-line program.
To see a complete list of the options supported for MFCC feature extraction,
execute the program featbin/compute-mfcc-feats with no arguments.
Recall that you saw some of these options being registered in 
the MfccOptions class, and others being registered in 
featbin/compute-mfcc-feats.cc.  The way to specify options is --option=value.

Next look at tree/build-tree.h.  Find the BuildTree function.  This is the main
top-level function for building the decision tree.  Notice that it returns a
pointer the type EventMap.  This is a type that stores a function from a set of
(key, value) pairs to an integer.  It's defined in tree/event-map.h.  The keys
and values are both integers, but the keys represent phonetic-context positions
(typically 0, 1 or 2) and the values represent phones.  There is also a special
key, -1, that roughly represents the position in the HMM.  There are a lot of
details inside the tree-building code: list all the header files in the tree
directory.  For now we won't discuss it further.

Next look at hmm/hmm-topology.h.  The class HmmTopology defines a set of HMM
topologies for a number of phones.  In general each phone can have a different
topology.  The topology includes "default" transitions, used for initialization.
Look at the example topology in the extended comment at the top of the header.
There is a tag <PdfClass> (note: this format is vaguely XML-like, but it is not
really XML).  The <PdfClass> is always the same as the HMM-state (<State>).  In
general it doesn't have to be.  This is a mechanism to enforce tying of
distributions between distinct HMM states; it's possibly useful if you want to
create more interesting transition models.




*/
